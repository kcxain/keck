"""
CUDA vs PyTorch 正确性验证 + 性能测试模板

执行时需要以下文件在同一目录：
- model.py: PyTorch 基线实现，包含 Model, get_inputs, get_init_inputs
- model_new_patch.py: 重写后的 CUDA 实现，包含 ModelNew
- *.so: 编译好的 CUDA 扩展
"""

import gc
import os
import time

import torch

from model import Model, get_inputs, get_init_inputs

import model_new_patch
# 优先使用 ModelNew，如果没有再使用 Model
if hasattr(model_new_patch, 'ModelNew'):
    ModelNew = model_new_patch.ModelNew
elif hasattr(model_new_patch, 'Model'):
    ModelNew = model_new_patch.Model
else:
    raise ImportError("Neither 'ModelNew' nor 'Model' found in model_new_patch")

# 是否启用详细 profiler（环境变量控制，默认关闭以提升速度）
ENABLE_PROFILER = os.getenv("ENABLE_PROFILER", "0") == "1"

def set_seed(seed: int):  
    torch.manual_seed(seed)  
    torch.cuda.manual_seed(seed)

def _cleanup_gpu():
    """清理 GPU 缓存"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        torch.cuda.synchronize()


def transform_tensors(tensors, fn):
    """递归地对 tensor 应用变换函数"""
    if isinstance(tensors, torch.Tensor):
        return fn(tensors)
    if isinstance(tensors, dict):
        return {k: transform_tensors(v, fn) for k, v in tensors.items()}
    if isinstance(tensors, (list, tuple)):
        return type(tensors)(transform_tensors(t, fn) for t in tensors)
    return tensors


def check_equal(actual, expected):
    """检查两个输出是否在容差范围内相等"""
    # 统一转成 list
    if not isinstance(actual, (list, tuple)):
        actual = [actual]
    if not isinstance(expected, (list, tuple)):
        expected = [expected]

    # 检查长度一致
    assert len(actual) == len(expected), (
        f"Output count mismatch: {len(actual)} vs {len(expected)}"
    )

    for i, (x, y) in enumerate(zip(actual, expected)):
        torch.testing.assert_close(
            x, y, atol=1e-2, rtol=1e-2, msg=lambda m: f"Output {i} mismatch: {m}"
        )


def benchmark_model(model, inputs, warmup_runs=2, benchmark_runs=3):
    """Benchmark 模型执行时间"""
    # Warmup（不需要每次都 sync，最后统一 sync 一次）
    with torch.no_grad():
        for _ in range(warmup_runs):
            _ = model(*inputs)
    torch.cuda.synchronize()

    # Benchmark（使用 CUDA events 更精确）
    start_event = torch.cuda.Event(enable_timing=True)
    end_event = torch.cuda.Event(enable_timing=True)

    with torch.no_grad():
        start_event.record()
        for _ in range(benchmark_runs):
            _ = model(*inputs)
        end_event.record()

    torch.cuda.synchronize()
    elapsed_ms = start_event.elapsed_time(end_event)
    avg_time = elapsed_ms / 1000.0 / benchmark_runs

    # Profiler（可选，默认关闭）
    if ENABLE_PROFILER:
        with torch.profiler.profile(
            activities=[
                torch.profiler.ProfilerActivity.CPU,
                torch.profiler.ProfilerActivity.CUDA,
            ],
            record_shapes=False,  # 关闭以减少开销
            with_stack=False,  # 关闭以减少开销
        ) as prof:
            with torch.no_grad():
                _ = model(*inputs)
        print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
    else:
        # 简化输出，保持格式兼容
        print(f"[Profiler disabled] avg_time={avg_time * 1000:.3f}ms")

    return avg_time


def main():
    # 初始化模型
    init_inputs = get_init_inputs()
    if not isinstance(init_inputs, (list, tuple)):
        init_inputs = [init_inputs]
    set_seed(42)
    torch_model = Model(*init_inputs).cuda()
    set_seed(42)
    cuda_model = ModelNew(*init_inputs).cuda()

    # 复制权重
    # DONE: aligh with KernelBench, dropback to random seed
    try:
        cuda_model.load_state_dict(torch_model.state_dict())
    except Exception as e:
        pass

    # 准备输入（需要两份独立的副本，防止 in-place 操作互相影响）
    inputs = get_inputs()
    if not isinstance(inputs, (list, tuple)):
        inputs = [inputs]
    torch_inputs = transform_tensors(inputs, lambda x: x.cuda())
    cuda_inputs = transform_tensors(torch_inputs, lambda x: x.clone())
    # 此时cuda_inputs里的tensor会和torch_inputs一样，存储在GPU上（cuda设备上），因为.clone()不会更改设备，仅复制内容。

    # 正确性检查
    with torch.no_grad():
        # 先执行 CUDA 模型并立即检查错误
        try:
            cuda_outputs = cuda_model(*cuda_inputs)
            torch.cuda.synchronize()  # 强制同步，让 CUDA kernel 错误在这里暴露
        except RuntimeError as e:
            if "illegal memory access" in str(e) or "CUDA error" in str(e):
                raise RuntimeError(f"CUDA model execution failed: {e}") from e
            raise

        # 再执行 torch 模型
        torch_outputs = torch_model(*torch_inputs)
        torch.cuda.synchronize()

        # 对比结果
        check_equal(cuda_outputs, torch_outputs)

    print("#### Correctness check passed!")

    # Benchmark
    print("#### Benchmark Torch Start")
    torch_time = benchmark_model(torch_model, torch_inputs)
    print("#### Benchmark Troch End")  # 注意：保持原拼写以兼容 parse_eval_msg

    print("#### Benchmark CUDA Start")
    cuda_time = benchmark_model(cuda_model, cuda_inputs)
    print("#### Benchmark CUDA End")

    speedup = torch_time / cuda_time if cuda_time > 0 else 0
    print(
        f"Torch time: {torch_time:.6f}s, CUDA time: {cuda_time:.6f}s, Speedup: {speedup:.2f}x"
    )

    # 清理
    del cuda_model, torch_model, torch_inputs, cuda_inputs
    del cuda_outputs, torch_outputs
    gc.collect()
    torch.cuda.empty_cache()


if __name__ == "__main__":
    main()
